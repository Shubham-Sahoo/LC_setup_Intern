from typing import Tuple, Optional
import numpy as np
import torch

from setcpp import enforce_smoothness
from envs.base_env import BaseEnv
from policies.actor import Actor

from data.synthia import Frame
import utils

ingr = utils.Ingredient("baseline")
ingr.add_config("config/baseline.yaml")


class BaselineActor(Actor):
    @ingr.capture
    def __init__(self, latency):
        self._latency = latency

        # parameters
        self._EXPANSION             = 0.3
        self._RECESSION_F           = 0.4  # recession for front curtain
        self._RECESSION_R           = 1.0  # recession for random curtain
        self._SMOOTHNESS            = 0.05
        self._LC_INTENSITY_THRESH_F = 200
        self._LC_INTENSITY_THRESH_R = 200

    @property
    def latency(self) -> float:
        return self._latency

    def init_action(self,
                    state: Frame) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Simulates convergence of this policy in a frozen frame where the front curtain converges to a smoothed out\
        version of the safety envelope
        """
        # compute safety envelope
        ranges = utils.safety_envelope(state)  # (C,)

        # enforce smoothness
        ranges = np.array(enforce_smoothness(ranges, self._SMOOTHNESS), dtype=np.float32)  # (C,)

        return ranges, None, False, {}

    def step(self,
             obs: BaseEnv.Observation) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            obs (BaseEnv.Observation): observations

        Returns:
            act (np.ndarray, dtype=np.float32, shape=(C,)): sampled actions.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        f_return, r_return = obs

        # Get hits on front curtain
        f_curtain = f_return.lc_ranges.copy()  # (C,)
        f_hits = utils.hits_from_lc_image(f_return.lc_image, ithresh=self._LC_INTENSITY_THRESH_F)  # (C,)

        # Get hits on random curtain
        r_curtain = r_return.lc_ranges.copy()  # (C,)
        r_hits = utils.hits_from_lc_image(r_return.lc_image, ithresh=self._LC_INTENSITY_THRESH_R)  # (C,)

        ############################################################################################################
        # Expand+Recede+Smooth frontier
        ############################################################################################################

        n_hits = ~ (f_hits | r_hits)  # (C,) no hit from either front or random curtain

        # Expansion
        f_curtain[n_hits] += self._EXPANSION

        # Recession
        f_curtain[f_hits] -= self._RECESSION_F
        f_curtain[r_hits] = r_curtain[r_hits] - self._RECESSION_R

        # Enforce smoothness
        f_curtain = np.array(enforce_smoothness(f_curtain, self._SMOOTHNESS), dtype=np.float32)  # (C,)

        action, logp_a, control, info = f_curtain, None, True, {}  # logp_a is None since this policy is deterministic
        return action, logp_a, control, info

    def forward(self,
                obs: torch.Tensor) -> torch.distributions.Distribution:
        """
        Args:
            obs (torch.Tensor): observation in torch tensors.

        Returns:
            pi (torch.distributions.Distribution): predicted action distribution of the policy.
        """
        raise NotImplementedError

    def evaluate_actions(self,
                         obs: torch.Tensor,
                         act: torch.Tensor) -> torch.Tensor:
        """
        Args:
            obs (torch.Tensor): observation in torch tensors.
            act (torch.Tensor): actions in torch tensors.

        Returns:
            logp_a (torch.Tensor): log probability of taking actions "act" by the actor under "obs", as a torch tensor.
        """
        raise NotImplementedError

    def reset(self):
        pass
