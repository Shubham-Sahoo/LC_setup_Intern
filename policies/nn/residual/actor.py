from dataclasses import dataclass
from typing import Tuple, Optional, List, Union
import numpy as np

import torch
import torch.nn as nn
import torch.distributions as td

from data.synthia import Frame
from envs.base_env import BaseEnv
from policies.actor import Actor
from policies.nn.residual.features import NNResidualFeatures, NNResidualFeaturizer, CurtainInfo
import utils

REGISTERED_NETWORK_CLASSES = {}

ingr = utils.Ingredient("nn_residual")
ingr.add_config("config/nn.yaml")


def register_network(cls, name=None):
    global REGISTERED_NETWORK_CLASSES
    if name is None:
        name = cls.__name__
    assert name not in REGISTERED_NETWORK_CLASSES, f"exist class: {REGISTERED_NETWORK_CLASSES}"
    REGISTERED_NETWORK_CLASSES[name] = cls
    return cls


def get_network_class(name):
    global REGISTERED_NETWORK_CLASSES
    assert name in REGISTERED_NETWORK_CLASSES, f"available class: {REGISTERED_NETWORK_CLASSES}"
    return REGISTERED_NETWORK_CLASSES[name]


class NNResidualActor(Actor):
    @ingr.capture
    def __init__(self,
                 thetas: np.ndarray,
                 base_policy: Actor,
                 network: str,
                 latency: float):
        """
        Args:
            thetas (np.ndarray, dtype=np.float32, shape=(C,)): thetas of the camera rays,
                in degrees and in increasing order in [-fov/2, fov/2]
            base_policy (Actor): a base policy that the residual policy policy uses.
            network (str): name of the network class.
            latency (int): latency in milliseconds.
        """

        self.network = get_network_class(network)()
        if torch.cuda.is_available():
            self.network = self.network.cuda()

        self.base_policy = base_policy
        self._latency = latency + self.base_policy.latency  # latency of residual as well as base policy

        self.featurizer = NNResidualFeaturizer(thetas)

        self.history: List[NNResidualActor.PrevOCA] = []

    @dataclass
    class PrevOCA:
        o: BaseEnv.Observation  # observation at time t-1
        c: CurtainInfo  # curtain info from observations time t-1
        a: np.ndarray  # action of base policy at time t-1 using o

    @property
    def latency(self) -> float:
        return self._latency

    @property
    def feat_dim(self) -> int:
        return self.featurizer.feat_dim

    def init_action(self,
                    state: Frame) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            state (Frame): the state of the world represented by a Frame from the gt_state_device.

        Returns:
            action (np.ndarray, dtype=np.float32, shape=(C,)): action, in terms of ranges for each camera ray.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        act, logp_a, control, info = self.base_policy.init_action(state)  # use base policy
        return act, None, False, {}

    @torch.no_grad()
    def step(self,
             obs: BaseEnv.Observation) -> Tuple[np.ndarray, Optional[np.ndarray], bool, dict]:
        """
        Args:
            obs (BaseEnv.Observation): observations

        Returns:
            act (np.ndarray, dtype=np.float32, shape=(C,)): sampled actions -- these are absolute, not residual.
            logp_a (Optional[np.ndarray]): log-probability of the sampled actions. None if sampling is deterministic.
            control (bool): whether this policy had control of taking the action at this timestep.
                            - this will be used to determine whether to evaluate the policy at this frame.
                            - another eg. is that these are the timesteps when nn-based policies will run the network.
            info (dict): auxiliary info generated by policy, such as a vector representation of the observation while
                         generating the action.
        """
        # convert obs to cinfo and get base policy action
        # all these quantities are for current timestep
        cinfo = self.featurizer.obs_to_curtain_info(obs)
        base_act, logp_a, _, _ = self.base_policy.step(obs)
        oca = NNResidualActor.PrevOCA(obs, cinfo, base_act)

        # first timestep, base action is the final action.
        if len(self.history) == 0:
            act = base_act
            self.history.append(oca)
            return act, None, False, {}

        cinfo_prev = self.history[0].c
        feat = self.featurizer.cinfos2feats(cinfo_prev, cinfo, base_act)
        self.history[0] = oca  # update history

        # action from network
        pi = self.forward(feat)  # (td.distribution, batch_shape=(1,), event_shape=(C,)
        act: torch.Tensor = pi.sample()  # (1, C)
        act: np.ndarray = act.squeeze(0).cpu().numpy()  # (C,)

        logp_a  = None
        control = True
        info    = dict(
            features=feat.to_numpy().squeeze(0),  # (F,)
            base_action=base_act  # (C,)
        )
        return act, logp_a, control, info

    def forward(self,
                feats: Union[np.ndarray, NNResidualFeatures]) -> td.Distribution:
        """
        Args:
            feats (Union[np.ndarray, NNResidualFeatures])): features.

        Returns:
            pi (td.Distribution, dtype=torch.float32, BS=(B,), ES=(C,)): predicted action distribution of the policy.
        """
        if type(feats) is not NNResidualFeatures:
            feats: NNResidualFeatures = NNResidualFeatures.from_numpy(feats)  # NNResidualFeatures

        pi: td.Distribution = self.network(feats)  # (td.Distribution, BS=(B,), ES=(C,))
        return pi

    def evaluate_actions(self,
                         feat: np.ndarray,
                         act : np.ndarray) -> torch.Tensor:
        """
        Args:
            feat (np.ndarray, dtype=np.float32, shape=(B, F)): batch of features.
            act (np.ndarray, dtype=np.float32, shape=(B, C)): batch of actions.

        Returns:
            logp_a (torch.Tensor): log probability of taking actions "act" by the actor under "obs", as a torch tensor.
        """
        feat = NNResidualFeatures.from_numpy(feat)  # batch_size = (B,)
        pi = self.forward(feat)  # (td.Distribution, batch_shape=(B,) event_shape=(C,))
        act = torch.from_numpy(act)  # (B, C)
        if torch.cuda.is_available():
            act = act.cuda()
        logp_a = pi.log_prob(act)  # (B,)
        return logp_a

    def reset(self):
        super().reset()
        self.history.clear()

########################################################################################################################
# region Convolutional Neural Networks
########################################################################################################################


@register_network
class CNN(nn.Module):
    """Performs 1D convolutions"""
    def __init__(self):
        super().__init__()

        self.conv1 = nn.Conv1d(22, 11, kernel_size=5, padding=2)
        self.conv2 = nn.Conv1d(11,  5, kernel_size=5, padding=2)

        self.mu    = nn.Conv1d(5,  1, kernel_size=5, padding=2)
        self.sigma = nn.Conv1d(5,  1, kernel_size=5, padding=2)

        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"{self.__class__.__name__}: has {num_params} parameters")

    def forward(self,
                feat: NNResidualFeatures) -> td.Distribution:
        """
        Args:
            feat (NNResidualFeatures): features with 2D tensors, where the first dimension is the batch

        Returns:
            pi (td.Distribution, dtype=torch.float32, event_shape=(C,)): distribution over actions
        """
        B = feat.c1.i.shape[0]  # batch size

        def pad_zeros(x: np.ndarray,
                      left: bool):
            """
            Args:
                x (np.ndarray, dtype=np.float32, shape=(B, 2): 2D vector to pad zeros to
                left (bool): whether to pad zeros to the left or not (right).
            """
            assert len(x.shape) == 2 and x.shape[0] == B
            if left:
                return np.hstack([np.zeros([B, 1]).astype(x.dtype), x])
            else:
                return np.hstack([x, np.zeros([B, 1]).astype(x.dtype)])

        eh1 = feat.c1[:, 1:] - feat.c1[:, :-1]
        eh2 = feat.c2[:, 1:] - feat.c2[:, :-1]
        ev  = feat.c2 - feat.c1

        x = np.stack([
                      # intensities
                      feat.c1.i,
                      feat.c2.i,
                      # horizontal edges 1
                      pad_zeros(eh1.x, left=True ),
                      pad_zeros(eh1.x, left=False),
                      pad_zeros(eh1.z, left=True ),
                      pad_zeros(eh1.z, left=False),
                      pad_zeros(eh1.r, left=True ),
                      pad_zeros(eh1.r, left=False),
                      pad_zeros(eh1.t, left=True ),
                      pad_zeros(eh1.t, left=False),
                      # horizontal edges 2
                      pad_zeros(eh2.x, left=True),
                      pad_zeros(eh2.x, left=False),
                      pad_zeros(eh2.z, left=True),
                      pad_zeros(eh2.z, left=False),
                      pad_zeros(eh2.r, left=True),
                      pad_zeros(eh2.r, left=False),
                      pad_zeros(eh2.t, left=True),
                      pad_zeros(eh2.t, left=False),
                      # vertical edges
                      ev.x,
                      ev.z,
                      ev.r,
                      # residual actions of base policy w.r.t curtain 2
                      feat.rb - feat.c2.r
                      ], axis=1)  # (B, 22, C)

        x = torch.from_numpy(x)  # (B, 22, C)
        if torch.cuda.is_available():
            x = x.cuda()

        x = self.conv1(x)  # (B, 6, C)
        x = torch.relu(x)
        x = self.conv2(x)  # (B, 3, C)
        x = torch.relu(x)

        # mu: absolute
        mu = self.mu(x).squeeze(1)   # (B, C)

        rb = torch.from_numpy(feat.rb)
        if torch.cuda.is_available():
            rb = rb.cuda()

        mu = mu + rb  # (B, C)

        # sigma = 1e-5 + torch.relu(self.sigma(x)).squeeze(1)  # (B, C)
        sigma = 1e-6

        pi = td.Normal(loc=mu, scale=sigma)  # batch_shape=[B, C] event_shape=[]
        pi = td.Independent(pi, 1)  # batch_shape=[B,] event_shape=[C,]
        return pi

# endregion
########################################################################################################################
